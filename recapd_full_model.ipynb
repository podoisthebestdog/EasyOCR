{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/podoisthebestdog/EasyOCR/blob/master/recapd_full_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RecapD 기반 텍스트-이미지 생성 모델"
      ],
      "metadata": {
        "id": "n6Y9Vygnz3nz"
      },
      "id": "n6Y9Vygnz3nz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. library import**"
      ],
      "metadata": {
        "id": "LYqAfwSnz7T4"
      },
      "id": "LYqAfwSnz7T4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-1. pytorch 신경망 구조 및 학습 핵심 모듈 import"
      ],
      "metadata": {
        "id": "4nuIJMRZ0IKG"
      },
      "id": "4nuIJMRZ0IKG"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "-Z0rFG0Gz3R4"
      },
      "id": "-Z0rFG0Gz3R4",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-2. 이미지 처리 및 coco 캡션 데이터셋 사용을 위한 torchvision tools"
      ],
      "metadata": {
        "id": "14uGfdnC0AQk"
      },
      "id": "14uGfdnC0AQk"
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CocoCaptions\n",
        "from torchvision.utils import save_image\n",
        "from torchvision.transforms.functional import resize\n",
        "from torchvision.models import inception_v3"
      ],
      "metadata": {
        "id": "9QOmeQ7G2QZl"
      },
      "id": "9QOmeQ7G2QZl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-3. normal utilities + 평가지표 계산용 CLIP + GUI용 streamlit"
      ],
      "metadata": {
        "id": "nSGTSUAC2XEs"
      },
      "id": "nSGTSUAC2XEs"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import clip\n",
        "import streamlit as st\n",
        "from torchvision.transforms import ToPILImage"
      ],
      "metadata": {
        "id": "gzESRVIl2hG2"
      },
      "id": "gzESRVIl2hG2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. 텍스트 전처리와 vocab 생성**"
      ],
      "metadata": {
        "id": "hB-eOBw42p6Z"
      },
      "id": "hB-eOBw42p6Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-1. 한글을 영어로 번역"
      ],
      "metadata": {
        "id": "7U6v4tqL22Wd"
      },
      "id": "7U6v4tqL22Wd"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tFK2zgCR24Qr"
      },
      "id": "tFK2zgCR24Qr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-2.텍스트를 소문자로 바꾸고 쉽표 제거 후 단어 단위로 나눔"
      ],
      "metadata": {
        "id": "gR8Z9MqG2vFv"
      },
      "id": "gR8Z9MqG2vFv"
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_tokenizer(text):\n",
        "    return text.strip().lower().replace('.', '').replace(',', '').split()"
      ],
      "metadata": {
        "id": "jZWGAO0V2kil"
      },
      "id": "jZWGAO0V2kil",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-3. COCO 캡션 전체에서 자주 나오는 단어를 기반으로 vocab(어휘사전)생성"
      ],
      "metadata": {
        "id": "TfwB1-D23Y5J"
      },
      "id": "TfwB1-D23Y5J"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(dataset, tokenizer, min_freq=1):\n",
        "    word_freq = defaultdict(int)\n",
        "    for _, captions in dataset:\n",
        "        for caption in captions:\n",
        "            for token in tokenizer(caption):\n",
        "                word_freq[token] += 1\n",
        "    #특수토큰 포함\n",
        "    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
        "    index = 4\n",
        "    for word, freq in word_freq.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[word] = index\n",
        "            index += 1\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "wPElAbk43iVS"
      },
      "id": "wPElAbk43iVS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. coco data set**"
      ],
      "metadata": {
        "id": "H3AP9Noe3q_M"
      },
      "id": "H3AP9Noe3q_M"
    },
    {
      "cell_type": "markdown",
      "source": [
        "이미지와 캡션을 하나의 텍스트-이미지 쌍으로 불러옴\n",
        "\n",
        "캡션을 토큰화하고 vocab index로 변환하여 고정 길이 시퀀스 생성"
      ],
      "metadata": {
        "id": "E-jzJkXG4DbI"
      },
      "id": "E-jzJkXG4DbI"
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoText2ImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, ann_file, tokenizer, vocab, transform=None, max_length=20):\n",
        "        self.dataset = CocoCaptions(root=image_dir, annFile=ann_file, transform=transform)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab = vocab\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, captions = self.dataset[idx]\n",
        "        caption = captions[0]\n",
        "        tokens = self.tokenizer(caption.lower())[:self.max_length]\n",
        "        token_ids = [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]\n",
        "        token_ids = [self.vocab['<sos>']] + token_ids + [self.vocab['<eos>']]\n",
        "        token_ids += [self.vocab['<pad>']] * (self.max_length + 2 - len(token_ids))\n",
        "        return image, torch.tensor(token_ids)"
      ],
      "metadata": {
        "id": "PFDXq7AE34Qm"
      },
      "id": "PFDXq7AE34Qm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. 텍스트인코더**"
      ],
      "metadata": {
        "id": "hmo3RIAz4Bwq"
      },
      "id": "hmo3RIAz4Bwq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "BiLSTM 기반 문장 인코더\n",
        "\n",
        "단어 시퀀스를 받아 양방향으로 처리한 후 마지막 hidden state를 결합하여 문장 임베딩 생성"
      ],
      "metadata": {
        "id": "Rrs-kGrQ4avL"
      },
      "id": "Rrs-kGrQ4avL"
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, captions):\n",
        "        embeds = self.embedding(captions)\n",
        "        _, (h_n, _) = self.lstm(embeds)\n",
        "        return torch.cat((h_n[0], h_n[1]), dim=-1)"
      ],
      "metadata": {
        "id": "Y37fl-lb4coT"
      },
      "id": "Y37fl-lb4coT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. IMAGE GENERATOR**"
      ],
      "metadata": {
        "id": "B0s4Z-y24f_V"
      },
      "id": "B0s4Z-y24f_V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장 벡터와 랜덤 노이즈 벡터를 입력받아 이미지를 생성\n",
        "\n",
        "Linear로 시작해 4×4 텐서를 만든 후 ConvTranspose2d로 업샘플링\n",
        "\n"
      ],
      "metadata": {
        "id": "QgHVHzgY4luL"
      },
      "id": "QgHVHzgY4luL"
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, text_dim, z_dim, ngf):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc = nn.Linear(text_dim + z_dim, ngf * 8 * 4 * 4)\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 2, 3, 4, 2, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, text_feat, z):\n",
        "        x = torch.cat((text_feat, z), dim=1)\n",
        "        x = self.fc(x).view(-1, 512, 4, 4)\n",
        "        return self.conv_blocks(x)\n"
      ],
      "metadata": {
        "id": "DjXf51nz4jhY"
      },
      "id": "DjXf51nz4jhY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6.Discriminator + Captioning Head**"
      ],
      "metadata": {
        "id": "dhSOIFp54pEb"
      },
      "id": "dhSOIFp54pEb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "6-1. 이미지 특정 맵으로 인코딩_CNN 기반"
      ],
      "metadata": {
        "id": "6HNXKBUR4zfT"
      },
      "id": "6HNXKBUR4zfT"
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, ndf):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, ndf, 4, 2, 1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(ndf, ndf*2, 4, 2, 1),\n",
        "            nn.BatchNorm2d(ndf*2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(ndf*2, ndf*4, 4, 2, 1),\n",
        "            nn.BatchNorm2d(ndf*4),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(ndf*4, ndf*8, 4, 2, 1),\n",
        "            nn.BatchNorm2d(ndf*8),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "HtEdZtPO4vCz"
      },
      "id": "HtEdZtPO4vCz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6-2. 이미지 특징을 입력받아 캡션 단어를 순서대로 생성_Transformer 기반 캡셔닝 모듈"
      ],
      "metadata": {
        "id": "Tm5gp5Qu45Ow"
      },
      "id": "Tm5gp5Qu45Ow"
    },
    {
      "cell_type": "code",
      "source": [
        "class CaptionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_len=20):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, embed_dim))\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
        "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, tgt_seq, memory):\n",
        "        tgt_embed = self.embedding(tgt_seq) + self.pos_embedding[:, :tgt_seq.size(1), :]\n",
        "        tgt_embed = tgt_embed.permute(1, 0, 2)\n",
        "        memory = memory.permute(1, 0, 2)\n",
        "        tgt_mask = nn.Transformer().generate_square_subsequent_mask(tgt_seq.size(1)).to(tgt_seq.device)\n",
        "        output = self.transformer_decoder(tgt_embed, memory, tgt_mask)\n",
        "        return self.fc_out(output.permute(1, 0, 2))\n"
      ],
      "metadata": {
        "id": "PbzIproK5D2w"
      },
      "id": "PbzIproK5D2w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6-3. 판별자(Discriminator) + 이미지 캡셔닝을 함께 수행\n",
        "\n",
        "생성된 이미지가 텍스트와 일치하는지 판별 + 캡션까지 생성하여 일치성 강화"
      ],
      "metadata": {
        "id": "XZz_hwIE5Edz"
      },
      "id": "XZz_hwIE5Edz"
    },
    {
      "cell_type": "code",
      "source": [
        "class RecapD(nn.Module):\n",
        "    def __init__(self, text_dim, ndf, vocab_size, cap_embed=256):\n",
        "        super().__init__()\n",
        "        self.image_encoder = ImageEncoder(ndf)\n",
        "        self.fc_text = nn.Linear(text_dim, ndf * 8)\n",
        "        self.disc_head = nn.Conv2d(ndf * 8 + ndf * 8, 1, 4)\n",
        "        self.caption_decoder = CaptionDecoder(vocab_size, cap_embed, 4, 512, 2)\n",
        "        self.fc_vis2seq = nn.Linear(ndf * 8 * 4 * 4, cap_embed)\n",
        "\n",
        "    def forward(self, image, text_feat, tgt_seq):\n",
        "        v = self.image_encoder(image)\n",
        "        t = self.fc_text(text_feat).unsqueeze(2).unsqueeze(3).expand_as(v)\n",
        "        joint = torch.cat((v, t), dim=1)\n",
        "        disc_score = self.disc_head(joint).view(-1)\n",
        "        memory = self.fc_vis2seq(v.view(v.size(0), -1).unsqueeze(1))\n",
        "        cap_logits = self.caption_decoder(tgt_seq, memory)\n",
        "        return disc_score, cap_logits"
      ],
      "metadata": {
        "id": "nr1LfgAm5E1o"
      },
      "id": "nr1LfgAm5E1o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. 평가지표 FID, R-Precision**\n",
        "\n"
      ],
      "metadata": {
        "id": "yF-Byg9w5Pmg"
      },
      "id": "yF-Byg9w5Pmg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "7-1.FID_생성 이미지와 실제 이미지의 분포 차이를 측정하는 지표 (Fréchet Inception Distance)"
      ],
      "metadata": {
        "id": "Ig4a-5Rj5VgU"
      },
      "id": "Ig4a-5Rj5VgU"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_fid(fake_features, real_features):\n",
        "    mu1, mu2 = fake_features.mean(0), real_features.mean(0)\n",
        "    sigma1, sigma2 = torch.cov(fake_features.T), torch.cov(real_features.T)\n",
        "    diff = mu1 - mu2\n",
        "    covmean = torch.linalg.sqrtm((sigma1 @ sigma2).cpu()).real\n",
        "    fid = diff.dot(diff) + torch.trace(sigma1 + sigma2 - 2 * covmean)\n",
        "    return fid.item()"
      ],
      "metadata": {
        "id": "rbu49QsO5J3r"
      },
      "id": "rbu49QsO5J3r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7-2.생성 이미지와 텍스트의 일치도를 평가\n",
        "\n",
        "CLIP 모델로 이미지/텍스트 임베딩을 구하고 cosine 유사도로 측정"
      ],
      "metadata": {
        "id": "bIW9GQuM5fzS"
      },
      "id": "bIW9GQuM5fzS"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_r_precision(model, image_tensor_list, text_list, device):\n",
        "    model.eval()\n",
        "    image_features, text_features = [], []\n",
        "    with torch.no_grad():\n",
        "        for img in image_tensor_list:\n",
        "            img = resize(img, (224, 224)).unsqueeze(0).to(device)\n",
        "            image_feat = model.encode_image(img).cpu()\n",
        "            image_features.append(image_feat)\n",
        "        for text in text_list:\n",
        "            text_tokens = clip.tokenize([text]).to(device)\n",
        "            text_feat = model.encode_text(text_tokens).cpu()\n",
        "            text_features.append(text_feat)\n",
        "    sims = cosine_similarity(torch.cat(image_features), torch.cat(text_features))\n",
        "    ranks = sims.argsort(axis=1)[:, -1]\n",
        "    return np.mean([i == rank for i, rank in enumerate(ranks)])\n"
      ],
      "metadata": {
        "id": "-M96ch7T5iwi"
      },
      "id": "-M96ch7T5iwi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**8. Steamlit GUI**"
      ],
      "metadata": {
        "id": "NFMJ1iK05lbA"
      },
      "id": "NFMJ1iK05lbA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "8-1. 사용자가 입력한 텍스트로부터 이미지를 생성하는 함수"
      ],
      "metadata": {
        "id": "pUgFzFjX503m"
      },
      "id": "pUgFzFjX503m"
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate_image(model, text_encoder, generator, vocab, tokenizer, text, device):\n",
        "    model.eval()\n",
        "    generator.eval()\n",
        "    token_ids = [vocab.get(tok, vocab['<unk>']) for tok in tokenizer(text.lower())]\n",
        "    token_ids = [vocab['<sos>']] + token_ids + [vocab['<eos>']]\n",
        "    token_ids += [vocab['<pad>']] * (22 - len(token_ids))\n",
        "    tokens = torch.tensor(token_ids).unsqueeze(0).to(device)\n",
        "    text_feat = text_encoder(tokens)\n",
        "    z = torch.randn(1, 100).to(device)\n",
        "    fake_image = generator(text_feat, z)[0].cpu()\n",
        "    return ToPILImage()(fake_image.add(1).div(2).clamp(0, 1))"
      ],
      "metadata": {
        "id": "ntLvDfFy5lpA"
      },
      "id": "ntLvDfFy5lpA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8-2. Streamlit을 사용해 간단한 웹 앱 GUI 구성\n",
        "\n",
        "텍스트 입력창, 버튼, 생성 이미지 출력 포함\n"
      ],
      "metadata": {
        "id": "6yJ4MCXl56nK"
      },
      "id": "6yJ4MCXl56nK"
    },
    {
      "cell_type": "code",
      "source": [
        "def run_gui(text_encoder, generator, vocab, tokenizer, device):\n",
        "    st.title(\"RecapD 텍스트-이미지 생성기\")\n",
        "    user_text = st.text_input(\"텍스트 설명을 입력하세요:\", \"a yellow fire hydrant on the sidewalk\")\n",
        "    if st.button(\"이미지 생성\"):\n",
        "        image = generate_image(None, text_encoder, generator, vocab, tokenizer, user_text, device)\n",
        "        st.image(image, caption=\"생성된 이미지\", use_column_width=True)"
      ],
      "metadata": {
        "id": "gQcqjKFr5qw0"
      },
      "id": "gQcqjKFr5qw0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. 실행예시**"
      ],
      "metadata": {
        "id": "BQBPkBY15_JU"
      },
      "id": "BQBPkBY15_JU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "실제 실행을 위해 필요한 코드\n",
        "\n",
        "학습된 모델 불러오고 run_gui를 실행하면 GUI 사용 가능"
      ],
      "metadata": {
        "id": "mMHQpMZ36FNs"
      },
      "id": "mMHQpMZ36FNs"
    },
    {
      "cell_type": "code",
      "source": [
        "# 실행 예시:\n",
        "# if __name__ == '__main__':\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     run_gui(text_encoder, generator, vocab, simple_tokenizer, device)\n"
      ],
      "metadata": {
        "id": "Cu0GbQCY6C_r"
      },
      "id": "Cu0GbQCY6C_r",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}